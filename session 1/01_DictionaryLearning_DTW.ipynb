{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGrULaK82Mtx"
      },
      "source": [
        "**Machine Learning for Time Series (Master MVA)**\n",
        "\n",
        "- TP1, Thursday 19<sup>th</sup> October 2023\n",
        "- [Link to the class material.](http://www.laurentoudre.fr/ast.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEF4nFUn2Mty"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we illustrate two concepts:\n",
        "- convolutional dictionary learning (CDL),\n",
        "- dynamic time warping (DTW)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oSx_wHX2Mty"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrKzMRmK2Mty"
      },
      "source": [
        "**Import**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/alphacsc/alphacsc.git"
      ],
      "metadata": {
        "id": "neJBP7IF278O",
        "outputId": "7f966554-b47a-46d5-e55a-6f7b6a9d20eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/alphacsc/alphacsc.git\n",
            "  Cloning https://github.com/alphacsc/alphacsc.git to /tmp/pip-req-build-gtow_y5m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/alphacsc/alphacsc.git /tmp/pip-req-build-gtow_y5m\n",
            "  Resolved https://github.com/alphacsc/alphacsc.git to commit 21496d7a1703332208aefbb07bea78965a5258d9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mne (from alphacsc==0.4.1.dev20)\n",
            "  Downloading mne-1.5.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (1.11.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (1.3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from alphacsc==0.4.1.dev20) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->alphacsc==0.4.1.dev20) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne->alphacsc==0.4.1.dev20) (4.66.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->alphacsc==0.4.1.dev20) (1.7.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne->alphacsc==0.4.1.dev20) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne->alphacsc==0.4.1.dev20) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->alphacsc==0.4.1.dev20) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->alphacsc==0.4.1.dev20) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->alphacsc==0.4.1.dev20) (3.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->alphacsc==0.4.1.dev20) (3.11.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->alphacsc==0.4.1.dev20) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->alphacsc==0.4.1.dev20) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne->alphacsc==0.4.1.dev20) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->alphacsc==0.4.1.dev20) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->alphacsc==0.4.1.dev20) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->alphacsc==0.4.1.dev20) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->alphacsc==0.4.1.dev20) (2023.7.22)\n",
            "Building wheels for collected packages: alphacsc\n",
            "  Building wheel for alphacsc (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alphacsc: filename=alphacsc-0.4.1.dev20-py3-none-any.whl size=101471 sha256=2c023dd2f203fbceae2f35c7f0e44f90669522c7cc037aea82079d4f18e24535\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bijyrvqt/wheels/4a/4c/7e/92942fddcba51c33fb20c02a4cb984aa2274f0f93a4ec3a481\n",
            "Successfully built alphacsc\n",
            "Installing collected packages: mne, alphacsc\n",
            "Successfully installed alphacsc-0.4.1.dev20 mne-1.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtw-python"
      ],
      "metadata": {
        "id": "hE5oZh3321z1",
        "outputId": "e1abdfca-d2bb-4e48-f1bf-ad53615d752d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtw-python\n",
            "  Downloading dtw_python-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.5/645.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from dtw-python) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from dtw-python) (1.11.3)\n",
            "Installing collected packages: dtw-python\n",
            "Successfully installed dtw-python-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loadmydata"
      ],
      "metadata": {
        "id": "wiCj-2z73IPr",
        "outputId": "ba2360ce-17cd-46b9-c376-6f6899e53a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loadmydata\n",
            "  Downloading loadmydata-0.0.10.tar.gz (19 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from loadmydata) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from loadmydata) (1.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from loadmydata) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from loadmydata) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from loadmydata) (1.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from loadmydata) (4.66.1)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.10/dist-packages (from loadmydata) (1.9.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->loadmydata) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.23.0->loadmydata) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->loadmydata) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->loadmydata) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->loadmydata) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->loadmydata) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->loadmydata) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->loadmydata) (2023.7.22)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl->loadmydata) (6.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->loadmydata) (1.16.0)\n",
            "Building wheels for collected packages: loadmydata\n",
            "  Building wheel for loadmydata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for loadmydata: filename=loadmydata-0.0.10-py3-none-any.whl size=18028 sha256=f6ada0f7055aec3f3965d340dac64888c3c3a1216b5f74203039e427a0ce8686\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/c0/45/fbba10ccd037a6a95790143395becb128b59ff14ea71603af7\n",
            "Successfully built loadmydata\n",
            "Installing collected packages: loadmydata\n",
            "Successfully installed loadmydata-0.0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDan4Jvi2Mty",
        "outputId": "a775df93-9203-432a-bbbc-74594902988d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/alphacsc/utils/compute_constants.py:28: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 1d, A), array(float64, 1d, A))\n",
            "  DtD[k0, k, t0] = np.dot(uv[k0, n_channels:],\n",
            "/usr/local/lib/python3.10/dist-packages/alphacsc/utils/compute_constants.py:35: NumbaPerformanceWarning: np.dot() is faster on contiguous arrays, called on (array(float64, 2d, A), array(float64, 2d, A))\n",
            "  DtD *= np.dot(u, u.T).reshape(n_atoms, n_atoms, 1)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from dtw import dtw\n",
        "from IPython.display import Audio, display\n",
        "from loadmydata.load_uea_ucr import load_uea_ucr_data\n",
        "from matplotlib.colors import rgb2hex\n",
        "from scipy.cluster import hierarchy\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "from alphacsc import learn_d_z\n",
        "try:\n",
        "    from alphacsc.utils import construct_X\n",
        "except:\n",
        "    from alphacsc.utils.convolution import construct_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB2on5rb2Mtz"
      },
      "source": [
        "**Utility functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "poC2OKmo2Mtz"
      },
      "outputs": [],
      "source": [
        "def plot_CDL(signal, Z, D, figsize=(15, 10)):\n",
        "    \"\"\"Plot the learned dictionary `D` and the associated sparse codes `Z`.\n",
        "\n",
        "    `signal` is an univariate signal of shape (n_samples,) or (n_samples, 1).\n",
        "    \"\"\"\n",
        "    (atom_length, n_atoms) = np.shape(D)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.subplot(n_atoms + 1, 3, (2, 3))\n",
        "    plt.plot(signal)\n",
        "    for i in range(n_atoms):\n",
        "        plt.subplot(n_atoms + 1, 3, 3 * i + 4)\n",
        "        plt.plot(D[:, i])\n",
        "        plt.subplot(n_atoms + 1, 3, (3 * i + 5, 3 * i + 6))\n",
        "        plt.plot(Z[:, i])\n",
        "        plt.ylim((np.min(Z), np.max(Z)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rl4Bm7fS2Mtz"
      },
      "outputs": [],
      "source": [
        "def display_distance_matrix_as_table(\n",
        "    distance_matrix, labels=None, figsize=(8, 2)\n",
        "):\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.axis(\"tight\")\n",
        "    ax.axis(\"off\")\n",
        "    norm = mpl.colors.Normalize()\n",
        "    cell_colours_hex = np.empty(shape=distance_matrix.shape, dtype=object)\n",
        "    cell_colours_rgba = plt.get_cmap(\"magma\")(norm(distance_matrix))\n",
        "\n",
        "    for i in range(distance_matrix.shape[0]):\n",
        "        for j in range(i + 1, distance_matrix.shape[0]):\n",
        "            cell_colours_hex[i, j] = rgb2hex(\n",
        "                cell_colours_rgba[i, j], keep_alpha=True\n",
        "            )\n",
        "            cell_colours_hex[j, i] = cell_colours_hex[i, j]\n",
        "\n",
        "    if labels is not None:\n",
        "        _ = ax.table(\n",
        "            cellText=distance_matrix,\n",
        "            colLabels=labels,\n",
        "            rowLabels=labels,\n",
        "            loc=\"center\",\n",
        "            cellColours=cell_colours_hex,\n",
        "        )\n",
        "    else:\n",
        "        _ = ax.table(\n",
        "            cellText=distance_matrix,\n",
        "            loc=\"center\",\n",
        "            cellColours=cell_colours_hex,\n",
        "        )\n",
        "\n",
        "    return ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qRkrMaLq2Mt0"
      },
      "outputs": [],
      "source": [
        "def get_n_largest(\n",
        "    arr: np.ndarray, n_largest: int = 3\n",
        ") -> (np.ndarray, np.ndarray):\n",
        "    \"\"\"Return the n largest values and associated indexes of an array.\n",
        "\n",
        "    (In decreasing order of value.)\n",
        "    \"\"\"\n",
        "    indexes = np.argsort(arr)[-n_largest:][::-1]\n",
        "    if n_largest == 1:\n",
        "        indexes = np.array(indexes)\n",
        "    values = np.take(arr, indexes)\n",
        "    return values, indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ECq9uwoS2Mt0"
      },
      "outputs": [],
      "source": [
        "def fig_ax(figsize=(15, 5)):\n",
        "    return plt.subplots(figsize=figsize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG4wwIct2Mt0"
      },
      "source": [
        "# Convolutional dictionary learning (CDL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKP7nQVO2Mt0"
      },
      "source": [
        "## Data\n",
        "\n",
        "Task is to classify the nature of the heartbeat signal.\n",
        "\n",
        "Heart sound recordings were sourced from several contributors around the world, collected at either a clinical or nonclinical environment, from both healthy subjects and pathological patients.\n",
        "Each series represent the aplitude of the singal over time.\n",
        "\n",
        "Heart sound recordings were sourced from several contributors around the world, collected at either a clinical or nonclinical environment, from both healthy subjects and pathological patients.\n",
        "The heart sound recordings were collected from different locations on the body. The typical four locations are aortic area, pulmonic area, tricuspid area and mitral area, but could be one of nine different locations.\n",
        "The sounds were divided into two classes: normal and abnormal. The normal recordings were from healthy subjects and the abnormal ones were from patients with a confirmed cardiac diagnosis.\n",
        "The patients suffer from a variety of illnesses, but typically they are heart valve defects and coronary artery disease patients.\n",
        "Heart valve defects include mitral valve prolapse, mitral regurgitation, aortic stenosis and valvular surgery. All the recordings from the patients were generally labeled as abnormal.\n",
        "Both healthy subjects and pathological patients include both children and adults.\n",
        "\n",
        "Data was recorded at 2,000Hz.\n",
        "Data was truncated to the shortest instance.\n",
        "\n",
        "Original data can be found here:\n",
        "https://www.physionet.org/physiobank/database/challenge/2016/\n",
        "\n",
        "Original paper:\n",
        "Goldberger AL, Amaral LA, Glass L, Hausdorff JM, Ivanov PC, Mark RG, Mietus JE, Moody GB, Peng CK, Stanley HE. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation. 2000 Jun 13;101(23):e215-20.\n",
        "\n",
        "Correspondence should be addressed to Ary L. Goldberger:\n",
        "ary@astro.bidmc.harvard.edu\n",
        "\n",
        "Instances: 409\n",
        "\n",
        "Time series length: 18,530\n",
        "\n",
        "Classes:\n",
        "- Normal (110)\n",
        "- Abnormal (299)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "TsrEqEMn2Mt0",
        "outputId": "bb19eb31-2601-47cf-e4d1-42aac945077e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-94d9d5368b23>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BinaryHeartbeat.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m204\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18530\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Abnormal'\u001b[0m\u001b[0;3...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1339\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: BinaryHeartbeat.csv not found."
          ]
        }
      ],
      "source": [
        "X_train = np.loadtxt(\"BinaryHeartbeat.csv\").reshape((204, 18530, 1))\n",
        "y_train = np.array(['Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Abnormal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDLuTD6J2Mt1"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>What is the sampling frequency of those sounds?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdsU36gw2Mt1"
      },
      "outputs": [],
      "source": [
        "FREQUENCY = ...  # Hz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "souBUmIx2Mt1"
      },
      "source": [
        "We can plot a signal from each class (normal and abnormal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIob6CDl2Mt1"
      },
      "outputs": [],
      "source": [
        "fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(20, 3), sharey=True)\n",
        "fig.tight_layout()\n",
        "for ind, ax in zip([1, 200], ax_arr):\n",
        "    s = X_train[ind]\n",
        "    tt = np.arange(s.size) / FREQUENCY\n",
        "    ax.plot(tt, s)\n",
        "    ax.set_xlim(0, s.size / FREQUENCY)\n",
        "    ax.set_xlabel(\"Time (s)\")\n",
        "    _ = ax.set_title(f\"label: {data.y_train[ind]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2Fzs07p2Mt1"
      },
      "outputs": [],
      "source": [
        "# Since the time series are sound signals, we can choose one and listen to it.\n",
        "\n",
        "for ind in [1, 200]:\n",
        "    signal = X_train[ind]\n",
        "    label = data.y_train[ind]\n",
        "    print(label)\n",
        "    display(Audio(signal.flatten(), rate=FREQUENCY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-tOijis2Mt1"
      },
      "source": [
        "For the subsequent study, we select only 6 elements (3 from each classe) from the complete data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EirKPAUa2Mt1"
      },
      "outputs": [],
      "source": [
        "sub_sample = [0, 1, 2, 190, 191, 192]  # 3 Normal, 3 Abnormal\n",
        "X = np.take(\n",
        "    X_train, sub_sample, axis=0\n",
        ").squeeze()  # shape (n_series, n_samples)\n",
        "y = np.take(y_train, sub_sample, axis=0)  # shape (n_series,)\n",
        "\n",
        "# normalize signals (zero mean, unit variance).\n",
        "X -= X.mean(axis=1).reshape(-1, 1)\n",
        "X /= X.std(axis=1).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-G1GCDMR2Mt1"
      },
      "outputs": [],
      "source": [
        "ind = 2\n",
        "signal = X[ind]\n",
        "label = y[ind]\n",
        "\n",
        "fig, ax_arr = plt.subplots(nrows=1, ncols=2, figsize=(20, 3), sharey=True)\n",
        "fig.tight_layout()\n",
        "ax = ax_arr[0]\n",
        "\n",
        "n_samples = signal.size\n",
        "tt = np.arange(n_samples) / FREQUENCY\n",
        "ax.plot(tt, signal)\n",
        "ax.set_xlabel(\"Time (s)\")\n",
        "ax.set_xlim(0, n_samples / FREQUENCY)\n",
        "\n",
        "ax.set_title(f\"label: {label}\")\n",
        "\n",
        "ax = ax_arr[1]\n",
        "start, end = 5000, 7500  # change here to zoom somewhere else\n",
        "ax.plot(tt[start:end], signal[start:end])\n",
        "ax.set_xlim(tt[start], tt[end])\n",
        "ax.set_xlabel(\"Time (s)\")\n",
        "_ = ax.set_title(f\"Zoom on [{start/FREQUENCY:.2f}s, {end/FREQUENCY:.2f}s]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXT0qRaz2Mt1"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Roughly, what is the duration of the important phenomenon (the heartbeat)?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRRUM2LM2Mt2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LiQs6kb2Mt2"
      },
      "source": [
        "## CDL on a single signal\n",
        "\n",
        "For a 1D signal $\\mathbf{x}\\in\\mathbb{R}^N$ with $N$ samples, the convolutional dictionary learning tasks amounts to solving the following optimization problem:\n",
        "\n",
        "$$\n",
        "\\min_{(\\mathbf{d}_k)_k, (\\mathbf{z}_k)_k \\\\ \\lVert\\mathbf{d}_k\\rVert^2\\leq 1} \\quad (1/2)\\left\\lVert \\mathbf{x} - \\sum_{k=1}^K \\mathbf{z}_k * \\mathbf{d}_k \\right\\rVert^2 \\quad + \\quad\\lambda \\sum_{k=1}^K \\lVert\\mathbf{z}_k\\rVert_1\n",
        "$$\n",
        "\n",
        "where $\\mathbf{d}_k\\in\\mathbb{R}^L$ are the $K$ dictionary atoms (patterns), $\\mathbf{z}_k\\in\\mathbb{R}^{N-L+1}$ are activations signals, and $\\lambda>0$ is the sparsity constraint.\n",
        "\n",
        "This problem is not convex with respect to the couple $(\\mathbf{d}_k)_k, (\\mathbf{z}_k)_k$ but convex when the subproblems are taken individually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY-BIatA2Mt2"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>What are the parameters that a user must calibrate when using CDL?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rkwcJTM2Mt2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vSVqV1l2Mt2"
      },
      "source": [
        "We can now apply CDL on a single signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N4EeaRP2Mt2"
      },
      "outputs": [],
      "source": [
        "# Select a signal\n",
        "signal = X[2]\n",
        "data = signal[np.newaxis, :]  # shape (1, n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg0LvHyX2Mt2"
      },
      "outputs": [],
      "source": [
        "# Parameters to change\n",
        "n_atoms = 3  # K\n",
        "atom_length = 2000  # L\n",
        "penalty = 3  # lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHCmBlXj2Mt2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# learning a dictionary and codes\n",
        "pobj, _, d_hat, z_hat, _ = learn_d_z(\n",
        "    X=data,\n",
        "    n_atoms=n_atoms,\n",
        "    n_times_atom=atom_length,\n",
        "    reg=penalty,\n",
        "    n_iter=30,\n",
        "    n_jobs=4,\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qMTiQCw2Mt2"
      },
      "outputs": [],
      "source": [
        "# plot the results\n",
        "plot_CDL(signal, z_hat.T.squeeze(), d_hat.T.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4iO4lnO2Mt2"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>How does the number of activation evolve when the sparsity penalty changes?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mMFN6ckd2Mt3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy9WkTa42Mt3"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Looking at the sparse codes, can you tell:</p>\n",
        "    <ul>    \n",
        "    <li>How many times each atom is activated?</li>\n",
        "    <li>What is the compression rate (number of non-zero coefficients in the sparse codes / signal length)?</li>\n",
        "    </ul>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xAhfh5Ch2Mt3"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S04vbwZ22Mt3"
      },
      "source": [
        "Listen to the learned dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHuqErPe2Mt3"
      },
      "outputs": [],
      "source": [
        "for k, atom in enumerate(d_hat):\n",
        "    print(f\"Atom {k}\")\n",
        "    display(Audio(atom, rate=FREQUENCY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZOSFSq2Mt3"
      },
      "source": [
        "Now, let us look at the reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihaw7MmO2Mt3"
      },
      "outputs": [],
      "source": [
        "# Reconstruction with the dictionary and the sparse codes\n",
        "reconstruction = construct_X(z_hat, d_hat).squeeze()\n",
        "\n",
        "fig, ax = fig_ax()\n",
        "tt = np.arange(signal.shape[0])\n",
        "ax.plot(tt, signal, label=\"original\", alpha=0.5)\n",
        "ax.plot(tt, reconstruction, label=\"reconstructed\")\n",
        "\n",
        "ax.set_title(f\"Reconstruction MSE: {np.mean((signal - reconstruction)**2):.2e}\")\n",
        "\n",
        "_ = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNwRt-C2Mt7"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>How does reconstruction error evolve when the sparsity penalty changes?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "YEOvDKge2Mt7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m1BAFP32Mt7"
      },
      "outputs": [],
      "source": [
        "fig, ax_arr = plt.subplots(\n",
        "    nrows=n_atoms // 3 + 1,\n",
        "    ncols=3,\n",
        "    figsize=(20, 4 * (n_atoms // 3 + 1)),\n",
        "    sharey=True,\n",
        ")\n",
        "\n",
        "for k in range(n_atoms):\n",
        "    ax = ax_arr.flatten()[k]\n",
        "    reconstruted_with_one_atom = construct_X(\n",
        "        z_hat[k, np.newaxis, :, :], d_hat[k, np.newaxis, :]\n",
        "    ).squeeze()\n",
        "    ax.plot(range(start, end), signal[start:end])\n",
        "    ax.plot(\n",
        "        range(start, end),\n",
        "        reconstruted_with_one_atom[start:end],\n",
        "    )\n",
        "    ax.set_title(f\"Atom {k} only\")\n",
        "\n",
        "ax = ax_arr.flatten()[n_atoms]\n",
        "ax.plot(range(start, end), signal[start:end])\n",
        "ax.plot(\n",
        "    range(start, end),\n",
        "    reconstruction[start:end],\n",
        ")\n",
        "_ = ax.set_title(f\"All atoms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa5NXzWK2Mt7"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Rerun the dictionary learning and sparse coding. What do you observe on the motif shape? And the reconstruction error?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Zg7cgG762Mt7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPSYUCby2Mt7"
      },
      "source": [
        "## CDL on the whole data set\n",
        "\n",
        "In this section, we apply CDL on the whole data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V45or4DF2Mt7"
      },
      "outputs": [],
      "source": [
        "# In the following, we fix the number of atoms and their length\n",
        "n_atoms = 5\n",
        "atom_length = 1500\n",
        "penalty = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pFDbe6B2Mt7"
      },
      "source": [
        "Dictionary learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0H8x8aK2Mt8"
      },
      "outputs": [],
      "source": [
        "pobj, _, d_hat, z_hat, _ = learn_d_z(\n",
        "    X=X,\n",
        "    n_atoms=n_atoms,\n",
        "    n_times_atom=atom_length,\n",
        "    reg=penalty,\n",
        "    verbose=1,\n",
        "    n_jobs=8,\n",
        "    n_iter=30,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wthZrkjT2Mt8"
      },
      "source": [
        "Let us plot each of the learned atoms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEusxSAO2Mt8"
      },
      "outputs": [],
      "source": [
        "fig, ax_arr = plt.subplots(\n",
        "    nrows=n_atoms // 3 + 1,\n",
        "    ncols=3,\n",
        "    figsize=(20, 4 * (n_atoms // 3 + 1)),\n",
        "    sharey=True,\n",
        ")\n",
        "\n",
        "for k, (atom, ax) in enumerate(zip(d_hat, ax_arr.flatten())):\n",
        "    ax.plot(atom)\n",
        "    ax.set_xlim(0, atom.size)\n",
        "    ax.set_title(f\"Atom {k}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOOOWemC2Mt8"
      },
      "source": [
        "For each signal of the data set, we compute the number of non-zeros activations and the reconstruction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdXd71YO2Mt8"
      },
      "outputs": [],
      "source": [
        "for k, (label, signal) in enumerate(zip(y, X)):\n",
        "    codes = z_hat[:, k, :]\n",
        "    reconstruction = construct_X(codes[:, np.newaxis, :], d_hat).squeeze()\n",
        "    error = np.mean((signal - reconstruction) ** 2)\n",
        "    nnz_activations = (codes > 1e-3).sum()\n",
        "\n",
        "    # select the used atoms\n",
        "    most_used_atoms_activations, most_used_atoms_indexes = get_n_largest(\n",
        "        (codes > 1e-3).sum(axis=1), n_largest=1\n",
        "    )\n",
        "    most_used_atom_msg = \", \".join(\n",
        "        f\"{ind} ({acti*100/nnz_activations:.1f}%)\"\n",
        "        for (acti, ind) in zip(\n",
        "            most_used_atoms_activations, most_used_atoms_indexes\n",
        "        )\n",
        "    )\n",
        "    print(\n",
        "        f\"Label: {label}, MSE: {error:.2f}, non-zero activations: {nnz_activations}, most used atoms: {most_used_atom_msg}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlhrZW02Mt8"
      },
      "source": [
        "Using the `Audio` function, we can listen to the learned dictionary and the reconstructed signals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hWJszfz2Mt8"
      },
      "outputs": [],
      "source": [
        "for k, (label, signal) in enumerate(zip(y, X)):\n",
        "    codes = z_hat[:, k, :]\n",
        "    reconstruction = construct_X(codes[:, np.newaxis, :], d_hat).squeeze()\n",
        "\n",
        "    print(label)\n",
        "    print(\"\\tOriginal\", end=\" \")\n",
        "    display(Audio(signal.flatten(), rate=FREQUENCY))\n",
        "    print(\"\\tReconstruction\", end=\" \")\n",
        "    display(Audio(reconstruction, rate=FREQUENCY))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHXzA_MF2Mt8"
      },
      "source": [
        "The noise has been removed. For the worst approximated signal, some hearbeats have been skipped."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH0K4WC-2Mt9"
      },
      "source": [
        "# Distance between signals (DTW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eakkGTVQ2Mt9"
      },
      "source": [
        "## Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCZSq37D2Mt9"
      },
      "source": [
        "FiftyWords is a data set of word outlines taken from the George\n",
        "Washington library by T. Rath and used in the paper \"Word image\n",
        "matching using dynamic time warping\", CVPR 2003.\n",
        "\n",
        "Each case is a word. A series is formed by taking the height\n",
        "profile of the word.\n",
        "\n",
        "![image.png](attachment:8305b0d4-2097-497d-ae08-7a125fc7a2cf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIpj6aso2Mt9"
      },
      "source": [
        "Here, we only deal with the top profile (in red)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isz7SGca2Mt9"
      },
      "outputs": [],
      "source": [
        "profiles = np.loadtxt(\"dtw_X_train.csv\").reshape((450, 270, 1))\n",
        "labels = np.array(['9', '18', '1', '5', '4', '3', '25', '3', '48', '1', '12', '12', '47', '42', '31', '3', '7', '17', '12', '14', '1', '1', '8', '11', '1', '4', '6', '29', '13', '2', '6', '3', '1', '3', '13', '18', '13', '9', '5', '44', '2', '30', '3', '2', '8', '2', '9', '4', '9', '13', '5', '13', '2', '6', '39', '2', '10', '8', '2', '8', '30', '48', '4', '29', '10', '44', '5', '36', '19', '22', '32', '1', '7', '15', '2', '10', '3', '18', '3', '35', '39', '19', '3', '4', '10', '3', '12', '6', '40', '1', '39', '8', '19', '38', '1', '38', '1', '20', '7', '37', '9', '38', '30', '3', '37', '22', '16', '3', '27', '17', '28', '26', '21', '4', '4', '6', '18', '6', '30', '3', '14', '7', '35', '12', '21', '3', '4', '2', '17', '10', '49', '45', '3', '37', '36', '28', '8', '11', '7', '5', '47', '17', '1', '4', '2', '48', '12', '11', '9', '22', '25', '9', '32', '50', '47', '1', '44', '35', '17', '24', '18', '7', '1', '14', '8', '31', '30', '16', '5', '32', '11', '1', '3', '2', '3', '5', '13', '14', '12', '2', '46', '36', '6', '45', '2', '7', '7', '2', '3', '2', '1', '14', '2', '1', '2', '23', '6', '19', '23', '50', '1', '10', '7', '1', '2', '12', '13', '2', '36', '2', '12', '3', '1', '14', '1', '3', '45', '10', '2', '11', '2', '28', '21', '2', '1', '3', '3', '44', '2', '1', '7', '4', '17', '5', '3', '48', '35', '12', '1', '1', '14', '8', '16', '43', '21', '23', '1', '2', '42', '33', '27', '1', '2', '26', '2', '1', '15', '32', '2', '2', '20', '22', '18', '46', '5', '9', '47', '2', '18', '32', '2', '41', '34', '29', '45', '31', '14', '46', '21', '1', '1', '13', '37', '1', '1', '18', '4', '40', '2', '27', '19', '20', '7', '38', '20', '26', '43', '2', '17', '2', '32', '40', '10', '19', '44', '16', '19', '15', '2', '4', '24', '33', '3', '16', '4', '33', '6', '33', '1', '11', '43', '4', '3', '13', '4', '19', '2', '7', '1', '12', '20', '3', '1', '1', '2', '11', '4', '36', '7', '1', '35', '1', '19', '30', '1', '15', '5', '1', '6', '2', '6', '48', '10', '49', '28', '29', '23', '2', '2', '37', '6', '2', '1', '6', '11', '4', '6', '1', '26', '40', '34', '28', '6', '20', '39', '10', '2', '15', '1', '13', '5', '7', '2', '3', '12', '1', '13', '3', '9', '43', '16', '8', '4', '3', '33', '24', '3', '26', '1', '5', '1', '26', '21', '12', '23', '3', '32', '28', '46', '2', '20', '27', '34', '11', '13', '1', '9', '21', '2', '8', '1', '8', '2', '1', '36', '44', '15', '3', '13', '2', '23', '1', '9', '1', '7', '4', '7', '13', '35', '11', '22', '24', '1', '2', '1', '4', '15', '2', '5', '3'])\n",
        "\n",
        "# normalize signals (zero mean, unit variance).\n",
        "profiles -= profiles.mean(axis=1).reshape(-1, 1, 1)\n",
        "profiles /= profiles.std(axis=1).reshape(-1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-efBda12Mt9"
      },
      "outputs": [],
      "source": [
        "word_1, label_1 = profiles[0].flatten(), labels[0]\n",
        "word_2, label_2 = profiles[1].flatten(), labels[1]\n",
        "fig, ax = fig_ax()\n",
        "ax.plot(word_1, label=label_1)\n",
        "ax.plot(word_2, label=label_2)\n",
        "_ = plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IhY1aFk2Mt9"
      },
      "source": [
        "## DTW between two signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1tKF3ZA2Mt9"
      },
      "outputs": [],
      "source": [
        "alignment = dtw(word_1, word_2, keep_internals=True)\n",
        "fig, ax = fig_ax()\n",
        "ax.plot(word_1, label=label_1)\n",
        "ax.plot(word_2, label=label_2)\n",
        "plt.title(f\"DTW: {alignment.distance:.2f}\")\n",
        "_ = plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAfIwF8o2Mt-"
      },
      "outputs": [],
      "source": [
        "alignment.plot(type=\"threeway\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCweQ7_H2Mt-"
      },
      "outputs": [],
      "source": [
        "alignment.plot(type=\"twoway\", offset=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wptUtCPM2Mt-"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Write a function  which computes the DTW distance between two signals: <tt>get_dtw_distance(signal_1: np.ndarray, signal_2: np.ndarray)->float</tt>.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "u9ilJXJL2Mt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4p0Acox2Mt-"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Choose a word and plot the most similar and the most dissimilar, according to the DTW. In addition, print the associated labels.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c5iztuxX2Mt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qMUmJvOO2Mt-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0KiWVda2Mt-"
      },
      "source": [
        "## Clustering with DTW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPL5hyNG2Mt-"
      },
      "source": [
        "### Clustering a small subset\n",
        "\n",
        "Out of the whole data set, let us choose 6 word profiles from 2 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qzc7fJN2Mt-"
      },
      "outputs": [],
      "source": [
        "# select a few profiles with two different classes\n",
        "keep_mask = np.isin(labels, [\"31\", \"34\"])\n",
        "labels_sub = labels[keep_mask]\n",
        "profiles_sub = profiles[keep_mask]\n",
        "# reorder by label\n",
        "order_indexes = labels_sub.argsort()\n",
        "labels_sub = labels_sub[order_indexes]\n",
        "profiles_sub = profiles_sub[order_indexes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X55m_YtJ2Mt_"
      },
      "source": [
        "Compute the distance matrix $D$ of this smaller data set: $D_{ij} = d(x_i, x_j)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QDLAemA2Mt_"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "distance_matrix = np.zeros(\n",
        "    (profiles_sub.shape[0], profiles_sub.shape[0]), dtype=float\n",
        ")\n",
        "\n",
        "for row in range(profiles_sub.shape[0]):\n",
        "    for col in range(row + 1, profiles_sub.shape[0]):\n",
        "        distance_matrix[row, col] = get_dtw_distance(\n",
        "            profiles_sub[row], profiles_sub[col]\n",
        "        )\n",
        "        distance_matrix[col, row] = distance_matrix[row, col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiqfDMn42Mt_"
      },
      "outputs": [],
      "source": [
        "_ = display_distance_matrix_as_table(\n",
        "    np.round(distance_matrix, 2), labels=labels_sub\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6gMzvzY2Mt_"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Create the same plot (distance matrix) with the Euclidean distance instead of the DTW. What do you observe?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe6ExaDg2Mt_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45546NdN2Mt_"
      },
      "source": [
        "### Clustering on a larger subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYkt9nqj2Mt_"
      },
      "source": [
        "Using the DTW, we can cluster a large set of data (43 words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kdLIdK52Mt_"
      },
      "outputs": [],
      "source": [
        "# select a few profiles with two different classes\n",
        "keep_mask = np.isin(labels, [\"4\", \"6\", \"14\"])\n",
        "profiles_sub = profiles[keep_mask]\n",
        "labels_sub = labels[keep_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoZbVDCV2Mt_"
      },
      "source": [
        "Compute the distance matrix with the DTW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NygQlKCF2MuA"
      },
      "outputs": [],
      "source": [
        "# Instead of the previous double for loop, we can use scipy function pdist\n",
        "distance_matrix = pdist(\n",
        "    profiles_sub.squeeze(), metric=get_dtw_distance\n",
        ")  # condensed distance matrix\n",
        "# Compute linkage matrix using the 'Ward' criterion\n",
        "linkage = hierarchy.ward(distance_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY7DKEEB2MuA"
      },
      "source": [
        "Plot the linkage as a dendogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms9qJhYd2MuA"
      },
      "outputs": [],
      "source": [
        "fig, ax = fig_ax((20, 10))\n",
        "\n",
        "cut_threshold = 100\n",
        "\n",
        "dendro = hierarchy.dendrogram(\n",
        "    linkage,\n",
        "    ax=ax,\n",
        "    labels=labels_sub,\n",
        "    color_threshold=cut_threshold,\n",
        "    distance_sort=True,\n",
        ")\n",
        "ax.axhline(cut_threshold, ls=\"--\", color=\"k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BidGO9L2MuA"
      },
      "outputs": [],
      "source": [
        "plt.imshow(squareform(distance_matrix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVtOkkYJ2MuA"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>Do the same clustering with the Euclidean distance instead. Plot the linkage as a dendogram.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63uBG9Sm2MuA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T9zwdlJ2MuA"
      },
      "source": [
        "<div class=\"alert alert-success\" role=\"alert\">\n",
        "    <p><b>Question</b></p>\n",
        "    <p>In the previous dendrograms, change the <tt>cut_threshold</tt> argument to have homogeneous clusters (as much as possible).</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUNdfWfr2MuA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "toc-autonumbering": true,
    "toc-showmarkdowntxt": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}